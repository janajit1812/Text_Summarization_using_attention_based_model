{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ebdee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\dassh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "# Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Machine Learning\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "def review_wordlist(review, remove_stopwords=True):\n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "   #print(words)\n",
    "    return(words)\n",
    "nltk.download('popular')\n",
    "from bs4 import BeautifulSoup\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def review_sentences(review, tokenizer, remove_stopwords=True):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    print(raw_sentences)\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_wordlist(raw_sentence,\\\n",
    "                                            remove_stopwords))\n",
    "\n",
    "    #print(sentences)\n",
    "    # This returns the list of lists\n",
    "    return sentences\n",
    "from gensim.models import word2vec\n",
    "from statistics import mode\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "            #featureVec = np.multiply(featureVec,model[word])\n",
    "            #featureVec = statistics.median(featureVec)\n",
    "            print(word)\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    print(nwords) \n",
    "    return featureVec\n",
    "from statistics import mode\n",
    "def featureVecMethod_code(words, model_code, num_features_code):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features_code,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model_code.wv.index_to_key)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model_code.wv[word])\n",
    "            #featureVec = np.multiply(featureVec,model[word])\n",
    "            #featureVec = statistics.median(featureVec)\n",
    "            print(word)\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    print(nwords) \n",
    "    return featureVec\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model , num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs_code(reviews, model_code, num_features_code ):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features_code),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod_code(review, model_code ,num_features_code )\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f482421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "['A network of community computer centres, linked by wireless technology, is providing a helping hand for poor farmers in Peru.', 'The pilot scheme in the Huaral Valley, 80 kilometres north of the capital Lima, aims to offer the 6,000-strong community up-to-date information on agricultural market prices and trends.', 'The Agricultural Information Project for Farmers of the Chancay-Huaral Valley also provides vital links between local organisations in charge of water irrigation, enabling them to coordinate their actions.', 'The non-government organisation, Cepes (Peruvian Centre for Social Studies) led the $200,000 project, also backed by local institutions, the Education and Agriculture ministries, and European development organisations.', 'The Board of Irrigation Users which runs the computer centres, aims to make the network self-sustainable within three years, through the cash generated by using the telecentres as internet cafes.', \"It is a precious element in Peru's coastal areas, because it is so scarce, and therefore it is necessary to have proper irrigation systems to make the most of it, Mr Saldarriaga told the BBC News website.\"]\n",
      "['A network of community computer centres, linked by wireless technology, is providing a helping hand for poor farmers in Peru.', 'The pilot scheme in the Huaral Valley, 80 kilometres north of the capital Lima, aims to offer the 6,000-strong community up-to-date information on agricultural market prices and trends.', 'The Agricultural Information Project for Farmers of the Chancay-Huaral Valley also provides vital links between local organisations in charge of water irrigation, enabling them to coordinate their actions.', 'The non-government organisation, Cepes (Peruvian Centre for Social Studies) led the $200,000 project, also backed by local institutions, the Education and Agriculture ministries, and European development organisations.', 'The plan includes training on computers and internet skills for both operators and users of the system, said Carlos Saldarriaga, technical coordinator at Cepes.', 'The Board of Irrigation Users which runs the computer centres, aims to make the network self-sustainable within three years, through the cash generated by using the telecentres as internet cafes.', 'One of the key elements of the project is the Agricultural Information System, with its flagship huaral.', 'The system also helps the inhabitants of the Chancay-Huaral Valley to organise their vital irrigation systems.', 'It is a precious element in Peru`s coastal areas, because it is so scarce, and therefore it is necessary to have proper irrigation systems to make the most of it, Mr Saldarriaga told the BBC News website.', 'The information network also allows farmers to look beyond their own region, and share experiences with other colleagues from the rest of Peru and even around the world.', \"Cepes says the involvement of the farmers has been key in the project's success.\", 'Throughout the last three years, the people have provided a vital thrust to the project; they feel it belongs to them, said Mr Saldarriaga.', \"So far, the Huaral programme promoters say the experience has been very positive, and are already planning on spreading the model among other farmers' organisations in Peru.\", 'The Cepes researcher recalls what happened in Cuyo, a 50-family community with no electricity, during the construction of the local telecentre site.', \"It was already dark when the technicians realised they didn't have any light bulbs to test the generator, so they turned up to the local store to buy light bulbs, recalls Carlos Saldarriaga.\"]\n"
     ]
    }
   ],
   "source": [
    "L=[\"A network of community computer centres, linked by wireless technology, is providing a helping hand for poor farmers in Peru. The pilot scheme in the Huaral Valley, 80 kilometres north of the capital Lima, aims to offer the 6,000-strong community up-to-date information on agricultural market prices and trends. The Agricultural Information Project for Farmers of the Chancay-Huaral Valley also provides vital links between local organisations in charge of water irrigation, enabling them to coordinate their actions. The non-government organisation, Cepes (Peruvian Centre for Social Studies) led the $200,000 project, also backed by local institutions, the Education and Agriculture ministries, and European development organisations. The Board of Irrigation Users which runs the computer centres, aims to make the network self-sustainable within three years, through the cash generated by using the telecentres as internet cafes. It is a precious element in Peru's coastal areas, because it is so scarce, and therefore it is necessary to have proper irrigation systems to make the most of it, Mr Saldarriaga told the BBC News website. \"]\n",
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in L:\n",
    "    #print (review)\n",
    "    #print(\"\\n\")\n",
    "    sentences += review_sentences(review, tokenizer)\n",
    "\n",
    "L_summary=[\"A network of community computer centres, linked by wireless technology, is providing a helping hand for poor farmers in Peru. The pilot scheme in the Huaral Valley, 80 kilometres north of the capital Lima, aims to offer the 6,000-strong community up-to-date information on agricultural market prices and trends. The Agricultural Information Project for Farmers of the Chancay-Huaral Valley also provides vital links between local organisations in charge of water irrigation, enabling them to coordinate their actions. The non-government organisation, Cepes (Peruvian Centre for Social Studies) led the $200,000 project, also backed by local institutions, the Education and Agriculture ministries, and European development organisations. The plan includes training on computers and internet skills for both operators and users of the system, said Carlos Saldarriaga, technical coordinator at Cepes. The Board of Irrigation Users which runs the computer centres, aims to make the network self-sustainable within three years, through the cash generated by using the telecentres as internet cafes. One of the key elements of the project is the Agricultural Information System, with its flagship huaral. The system also helps the inhabitants of the Chancay-Huaral Valley to organise their vital irrigation systems. It is a precious element in Peru`s coastal areas, because it is so scarce, and therefore it is necessary to have proper irrigation systems to make the most of it, Mr Saldarriaga told the BBC News website. The information network also allows farmers to look beyond their own region, and share experiences with other colleagues from the rest of Peru and even around the world. Cepes says the involvement of the farmers has been key in the project's success. Throughout the last three years, the people have provided a vital thrust to the project; they feel it belongs to them, said Mr Saldarriaga. So far, the Huaral programme promoters say the experience has been very positive, and are already planning on spreading the model among other farmers' organisations in Peru. The Cepes researcher recalls what happened in Cuyo, a 50-family community with no electricity, during the construction of the local telecentre site. It was already dark when the technicians realised they didn't have any light bulbs to test the generator, so they turned up to the local store to buy light bulbs, recalls Carlos Saldarriaga.\"]\n",
    "   \n",
    "sentences_code=[]  \n",
    "for review in L_summary:\n",
    "    sentences_code += review_sentences(review, tokenizer)\n",
    "    # Importing the built-in logging module\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 500  # Word vector dimensionality\n",
    "min_word_count = 5 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10       # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "num_features_code = 500  # Word vector dimensionality\n",
    "min_word_count_code = 6 # Minimum word count\n",
    "num_workers_code = 4     # Number of parallel threads\n",
    "context_code = 10        # Context window size\n",
    "downsampling_code = 1e-3 # (0.001) Downsample setting for frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be67e85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 19:32:52,985 : INFO : collecting all words and their counts\n",
      "2023-05-16 19:32:52,985 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-05-16 19:32:52,986 : INFO : collected 90 word types from a corpus of 108 raw words and 6 sentences\n",
      "2023-05-16 19:32:52,986 : INFO : Creating a fresh vocabulary\n",
      "2023-05-16 19:32:52,987 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 0 unique words (0.0%% of original 90, drops 90)', 'datetime': '2023-05-16T19:32:52.987475', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-05-16 19:32:52,987 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 0 word corpus (0.0%% of original 108, drops 108)', 'datetime': '2023-05-16T19:32:52.987475', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-05-16 19:32:52,988 : INFO : deleting the raw counts dictionary of 90 items\n",
      "2023-05-16 19:32:52,989 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2023-05-16 19:32:52,989 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 0 word corpus (0.0%% of prior 0)', 'datetime': '2023-05-16T19:32:52.989465', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-05-16 19:32:52,989 : INFO : estimated required memory for 0 words and 500 dimensions: 0 bytes\n",
      "2023-05-16 19:32:52,990 : INFO : resetting layer weights\n",
      "2023-05-16 19:32:52,990 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-16T19:32:52.990465', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [106]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word2vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_word_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownsampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model_code \u001b[38;5;241m=\u001b[39m word2vec\u001b[38;5;241m.\u001b[39mWord2Vec(sentences_code,\\\n\u001b[0;32m     11\u001b[0m                           workers\u001b[38;5;241m=\u001b[39mnum_workers_code,\\\n\u001b[0;32m     12\u001b[0m                           vector_size\u001b[38;5;241m=\u001b[39mnum_features_code,\\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m                           sample\u001b[38;5;241m=\u001b[39mdownsampling_code,\n\u001b[0;32m     16\u001b[0m                           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# To make the model memory efficient\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Users\\dassh\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:426\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_total_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mF:\\Users\\dassh\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1041\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha \u001b[38;5;241m=\u001b[39m end_alpha \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_training_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1046\u001b[0m     msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     ),\n\u001b[0;32m   1051\u001b[0m )\n",
      "File \u001b[1;32mF:\\Users\\dassh\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1531\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffective \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m higher than previous training cycles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mkey_to_index:  \u001b[38;5;66;03m# should be set by `build_vocab`\u001b[39;00m\n\u001b[1;32m-> 1531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must first build vocabulary before training the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors):\n\u001b[0;32m   1533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must initialize vectors before training the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          vector_size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling,\n",
    "                          epochs=100)\n",
    "model_code = word2vec.Word2Vec(sentences_code,\\\n",
    "                          workers=num_workers_code,\\\n",
    "                          vector_size=num_features_code,\\\n",
    "                          min_count=min_word_count_code,\\\n",
    "                          window=context_code,\n",
    "                          sample=downsampling_code,\n",
    "                          epochs=100)\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "model_code.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model_code_name = \"300features_40minwords_10context_code\"\n",
    "model.save(model_name)\n",
    "model_code.save(model_code_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1b7e6b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['network', 'community', 'computer', 'centres', 'linked', 'wireless', 'technology', 'providing', 'helping', 'hand', 'poor', 'farmers', 'peru', 'pilot', 'scheme', 'huaral', 'valley', 'kilometres', 'north', 'capital', 'lima', 'aims', 'offer', 'strong', 'community', 'date', 'information', 'agricultural', 'market', 'prices', 'trends', 'agricultural', 'information', 'project', 'farmers', 'chancay', 'huaral', 'valley', 'also', 'provides', 'vital', 'links', 'local', 'organisations', 'charge', 'water', 'irrigation', 'enabling', 'coordinate', 'actions', 'non', 'government', 'organisation', 'cepes', 'peruvian', 'centre', 'social', 'studies', 'led', 'project', 'also', 'backed', 'local', 'institutions', 'education', 'agriculture', 'ministries', 'european', 'development', 'organisations', 'board', 'irrigation', 'users', 'runs', 'computer', 'centres', 'aims', 'make', 'network', 'self', 'sustainable', 'within', 'three', 'years', 'cash', 'generated', 'using', 'telecentres', 'internet', 'cafes', 'precious', 'element', 'peru', 'coastal', 'areas', 'scarce', 'therefore', 'necessary', 'proper', 'irrigation', 'systems', 'make', 'mr', 'saldarriaga', 'told', 'bbc', 'news', 'website']]\n",
      "Review 0 of 1\n",
      "network\n",
      "farmers\n",
      "peru\n",
      "huaral\n",
      "information\n",
      "information\n",
      "project\n",
      "farmers\n",
      "huaral\n",
      "also\n",
      "local\n",
      "project\n",
      "also\n",
      "local\n",
      "network\n",
      "peru\n",
      "saldarriaga\n",
      "17\n",
      "[['network', 'community', 'computer', 'centres', 'linked', 'wireless', 'technology', 'providing', 'helping', 'hand', 'poor', 'farmers', 'peru', 'pilot', 'scheme', 'huaral', 'valley', 'kilometres', 'north', 'capital', 'lima', 'aims', 'offer', 'strong', 'community', 'date', 'information', 'agricultural', 'market', 'prices', 'trends', 'agricultural', 'information', 'project', 'farmers', 'chancay', 'huaral', 'valley', 'also', 'provides', 'vital', 'links', 'local', 'organisations', 'charge', 'water', 'irrigation', 'enabling', 'coordinate', 'actions', 'non', 'government', 'organisation', 'cepes', 'peruvian', 'centre', 'social', 'studies', 'led', 'project', 'also', 'backed', 'local', 'institutions', 'education', 'agriculture', 'ministries', 'european', 'development', 'organisations', 'plan', 'includes', 'training', 'computers', 'internet', 'skills', 'operators', 'users', 'system', 'said', 'carlos', 'saldarriaga', 'technical', 'coordinator', 'cepes', 'board', 'irrigation', 'users', 'runs', 'computer', 'centres', 'aims', 'make', 'network', 'self', 'sustainable', 'within', 'three', 'years', 'cash', 'generated', 'using', 'telecentres', 'internet', 'cafes', 'one', 'key', 'elements', 'project', 'agricultural', 'information', 'system', 'flagship', 'huaral', 'system', 'also', 'helps', 'inhabitants', 'chancay', 'huaral', 'valley', 'organise', 'vital', 'irrigation', 'systems', 'precious', 'element', 'peru', 'coastal', 'areas', 'scarce', 'therefore', 'necessary', 'proper', 'irrigation', 'systems', 'make', 'mr', 'saldarriaga', 'told', 'bbc', 'news', 'website', 'information', 'network', 'also', 'allows', 'farmers', 'look', 'beyond', 'region', 'share', 'experiences', 'colleagues', 'rest', 'peru', 'even', 'around', 'world', 'cepes', 'says', 'involvement', 'farmers', 'key', 'project', 'success', 'throughout', 'last', 'three', 'years', 'people', 'provided', 'vital', 'thrust', 'project', 'feel', 'belongs', 'said', 'mr', 'saldarriaga', 'far', 'huaral', 'programme', 'promoters', 'say', 'experience', 'positive', 'already', 'planning', 'spreading', 'model', 'among', 'farmers', 'organisations', 'peru', 'cepes', 'researcher', 'recalls', 'happened', 'cuyo', 'family', 'community', 'electricity', 'construction', 'local', 'telecentre', 'site', 'already', 'dark', 'technicians', 'realised', 'light', 'bulbs', 'test', 'generator', 'turned', 'local', 'store', 'buy', 'light', 'bulbs', 'recalls', 'carlos', 'saldarriaga']]\n",
      "Review 0 of 1\n",
      "farmers\n",
      "peru\n",
      "huaral\n",
      "information\n",
      "information\n",
      "project\n",
      "farmers\n",
      "huaral\n",
      "also\n",
      "local\n",
      "irrigation\n",
      "cepes\n",
      "project\n",
      "also\n",
      "local\n",
      "saldarriaga\n",
      "cepes\n",
      "irrigation\n",
      "project\n",
      "information\n",
      "huaral\n",
      "also\n",
      "huaral\n",
      "irrigation\n",
      "peru\n",
      "irrigation\n",
      "saldarriaga\n",
      "information\n",
      "also\n",
      "farmers\n",
      "peru\n",
      "cepes\n",
      "farmers\n",
      "project\n",
      "project\n",
      "saldarriaga\n",
      "huaral\n",
      "farmers\n",
      "peru\n",
      "cepes\n",
      "local\n",
      "local\n",
      "saldarriaga\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in L:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "print(clean_train_reviews)  \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n",
    "clean_train_reviews_code = []\n",
    "for review in L_summary:\n",
    "    clean_train_reviews_code.append(review_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "print(clean_train_reviews_code)  \n",
    "trainDataVecs_code = getAvgFeatureVecs_code(clean_train_reviews_code, model_code, num_features_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3d71ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93885875\n",
      "0.35149583\n",
      "20.139227\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs_array=trainDataVecs[0]\n",
    "trainDataVecs_code_array=trainDataVecs_code[0]\n",
    "#print(trainDataVecs_array)\n",
    "#print(trainDataVecs_code_array)\n",
    "unit_vector_1 = trainDataVecs_array / np.linalg.norm(trainDataVecs_array)\n",
    "#print(unit_vector_1)\n",
    "unit_vector_2 = trainDataVecs_code_array / np.linalg.norm(trainDataVecs_code_array)\n",
    "#print(unit_vector_2)\n",
    "dot_product = np.dot(unit_vector_1, unit_vector_2)\n",
    "print(dot_product)\n",
    "angle = np.arccos(dot_product)\n",
    "print(angle)\n",
    "print(np.rad2deg(angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c50a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
